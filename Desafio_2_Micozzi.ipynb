{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f14487e",
      "metadata": {
        "id": "1f14487e"
      },
      "source": [
        "## Desafío 2 - Procesamiento del Lenguaje Natural I\n",
        "##### Docentes: Rodrigo Cárdenas / Nicolás  Vattuone\n",
        "##### Autora: María Luz Micozzi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26265587",
      "metadata": {
        "id": "26265587"
      },
      "source": [
        "- Crear sus propios vectores con Gensim\n",
        "basado en lo visto en clase con otro\n",
        "dataset.\n",
        "- Probar términos de interés y explicar\n",
        "similitudes en el espacio de embeddings.\n",
        "- Intentar plantear y probar tests de\n",
        "analogías.\n",
        "- Graficar los embeddings\n",
        "resultantes.\n",
        "- Sacar conclusiones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6KFy3HFASsi3",
      "metadata": {
        "id": "6KFy3HFASsi3"
      },
      "source": [
        "Se utilizará para este desafío el libro ***“Orgullo y Prejuicio”*** en español. El mismo fue descargado del sitio web [textos.info](https://www.textos.info/jane-austen/orgullo-y-prejuicio/descargar-pdf) y luego subido a [drive](https://drive.google.com/file/d/1oNRvQ4wl8CNbVdVkKyjZQuVm8kInl7Xg/view?usp=sharing) en formato txt."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade numpy==1.24.0\n",
        "# !pip install --upgrade --force-reinstall gensim"
      ],
      "metadata": {
        "id": "ajHfWa4HPssn"
      },
      "id": "ajHfWa4HPssn",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "l3rofywDXtyK",
      "metadata": {
        "id": "l3rofywDXtyK"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8FXkd-2kXcuq",
      "metadata": {
        "id": "8FXkd-2kXcuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c849b17-77cf-412d-8f8f-9b8217163f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo ya está descargado.\n"
          ]
        }
      ],
      "source": [
        "# Descargamos el libro desde drive\n",
        "# Link = https://drive.google.com/file/d/1oNRvQ4wl8CNbVdVkKyjZQuVm8kInl7Xg/view?usp=drive_link\n",
        "\n",
        "file_id = '1oNRvQ4wl8CNbVdVkKyjZQuVm8kInl7Xg'\n",
        "output = 'orgullo_y_prejuicio.txt'\n",
        "\n",
        "if not os.path.exists(output):\n",
        "    print(\"Descargando el archivo desde Google Drive...\")\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El archivo ya está descargado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fcBopwiTZnyI",
      "metadata": {
        "id": "fcBopwiTZnyI"
      },
      "outputs": [],
      "source": [
        "# Armamos el dataset utilizando salto de línea para separar las oraciones/docs\n",
        "# Limpiamos titulo y autora, líneas en blanco, nro de capítulo y nro de página\n",
        "\n",
        "def clean_lines(archivo_txt):\n",
        "    with open(archivo_txt, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Saltar las dos primeras líneas (título y autora)\n",
        "    lines = lines[2:]\n",
        "\n",
        "    # Regex para detectar líneas como \"Capítulo I\", \"Capítulo IX\", \"CAPÍTULO XXV\"\n",
        "    pattern_chapter = re.compile(r'^cap[ií]tulo\\s+[ivxlcdm]+$', re.IGNORECASE)\n",
        "\n",
        "    # Filtrar y limpiar líneas\n",
        "    clean_lines = []\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if not line or line.isdigit() or pattern_chapter.match(line):\n",
        "          continue\n",
        "\n",
        "      # Limpieza de guiones\n",
        "      line = re.sub(r'\\b-(\\w+)', r'\\1', line)\n",
        "      line = re.sub(r'(\\w+)-\\b', r'\\1', line)\n",
        "      line = line.replace('—', '')\n",
        "\n",
        "      clean_lines.append(line)\n",
        "\n",
        "    # Crear DataFrame\n",
        "    df = pd.DataFrame(clean_lines, columns=['line'])\n",
        "    return df\n",
        "\n",
        "\n",
        "df = clean_lines(\"orgullo_y_prejuicio.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "S8jTWCV-iD3d",
      "metadata": {
        "id": "S8jTWCV-iD3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff11b7e3-ff9b-4e7e-eb19-c38a2f59b5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Línea 0: Es una verdad mundialmente reconocida que un hombre soltero, poseedor\n",
            "Línea 1: de una gran fortuna, necesita una esposa.\n",
            "Línea 2: Sin embargo, poco se sabe de los sentimientos u opiniones de un hombre\n",
            "Línea 3: de tales condiciones cuando entra a formar parte de un vecindario. Esta\n",
            "Línea 4: verdad está tan arraigada en las mentes de algunas de las familias que lo\n"
          ]
        }
      ],
      "source": [
        "# Imprimimos las primeras líneas\n",
        "for i in range(5):\n",
        "    print(f\"Línea {i}: {df['line'][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eUexb29ViTdg",
      "metadata": {
        "id": "eUexb29ViTdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fecfc0-c9a8-4e7d-deed-0d8b16a73dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Línea 6784: dirigir la mirada a los objetos que le señalaban, no distinguía ninguna parte\n",
            "Línea 9368: que se abrió la puerta y entró la visita. Era lady Catherine de Bourgh.\n",
            "Línea 5239: actual relativa pobreza. Usted le negó el porvenir que, como bien debe\n",
            "Línea 4860: Tengo entendido que el señor Bingley no piensa volver a Netherfield.\n",
            "Línea 2932: adelante. Collins continuó:\n"
          ]
        }
      ],
      "source": [
        "# Imprimir algunas líneas aleatorias\n",
        "muestras = df.sample(n=5, random_state=42)\n",
        "\n",
        "for idx, linea in muestras.iterrows():\n",
        "    print(f\"Línea {idx}: {linea['line']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "l_S0LZTRacTF",
      "metadata": {
        "id": "l_S0LZTRacTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f5406e-06cb-42e9-eaf6-a68d9877f547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de documentos: 10403\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de documentos:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bcc3c1",
      "metadata": {
        "id": "a4bcc3c1"
      },
      "source": [
        "### Tokenización y Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords') # Descargar las listas de stopwords\n",
        "spanish_stopwords = set(stopwords.words('spanish')) # Obtener el conjunto de stopwords en español\n",
        "\n",
        "def tokenize_line(line):\n",
        "    # Convertir la línea en una lista de palabras (tokens),\n",
        "    # eliminando puntuación y símbolos definidos en `filters`\n",
        "    tokens = text_to_word_sequence(\n",
        "        line,\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n¡¿'\n",
        "    )\n",
        "    return [t for t in tokens if t not in spanish_stopwords]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkFX5isfchT4",
        "outputId": "0087c217-6873-41f0-cbf1-803a4c2b9b30"
      },
      "id": "GkFX5isfchT4",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "-Qi-yv0LvRfs",
      "metadata": {
        "id": "-Qi-yv0LvRfs"
      },
      "outputs": [],
      "source": [
        "# Tokenizar cada línea en palabras\n",
        "sentence_tokens = [tokenize_line(line) for line in df['line']]\n",
        "\n",
        "# Crearmos el modelo generador de vectores\n",
        "# modelo Skipgram\n",
        "w2v_model = Word2Vec(min_count=1,\n",
        "                     window=3,\n",
        "                     vector_size=50,\n",
        "                     negative=20,\n",
        "                     workers=1,\n",
        "                     sg=1)\n",
        "\n",
        "# Construir vocabulario\n",
        "w2v_model.build_vocab(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras más frecuentes\n",
        "\n",
        "all_words = [word for sentence in sentence_tokens for word in sentence]\n",
        "top_words = Counter(all_words).most_common(20)\n",
        "\n",
        "print(\"Top 20 palabras más frecuentes:\")\n",
        "for palabra, freq in top_words:\n",
        "    print(f\"{palabra}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvofYNsEdju3",
        "outputId": "7595338b-6eb4-4635-c0fa-820e1563f99e"
      },
      "id": "KvofYNsEdju3",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 palabras más frecuentes:\n",
            "elizabeth: 874\n",
            "darcy: 603\n",
            "señor: 492\n",
            "si: 480\n",
            "tan: 473\n",
            "bingley: 399\n",
            "señora: 370\n",
            "jane: 337\n",
            "bennet: 336\n",
            "dijo: 318\n",
            "usted: 284\n",
            "wickham: 260\n",
            "ser: 255\n",
            "señorita: 254\n",
            "collins: 230\n",
            "casa: 221\n",
            "aunque: 204\n",
            "dos: 198\n",
            "hermana: 198\n",
            "bien: 197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cd29ed00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd29ed00",
        "outputId": "2416e8e1-3cd4-4ee6-bfe0-036ce92d3694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de docs en el corpus: 10403\n",
            "Cantidad de words distintas en el corpus: 10292\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de filas/docs encontradas en el corpus\n",
        "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)\n",
        "\n",
        "# Cantidad de words encontradas en el corpus\n",
        "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45648a03",
      "metadata": {
        "id": "45648a03"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62af5962",
      "metadata": {
        "id": "62af5962"
      },
      "outputs": [],
      "source": [
        "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
        "# Sobrecargamos el callback para poder tener esta información\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3d1ef9aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d1ef9aa",
        "outputId": "2cd166f8-7bc7-4163-90db-54bb77dc7c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 1397766.875\n",
            "Loss after epoch 1: 695744.375\n",
            "Loss after epoch 2: 506255.25\n",
            "Loss after epoch 3: 467570.75\n",
            "Loss after epoch 4: 447949.5\n",
            "Loss after epoch 5: 435489.0\n",
            "Loss after epoch 6: 417991.25\n",
            "Loss after epoch 7: 401951.0\n",
            "Loss after epoch 8: 398862.5\n",
            "Loss after epoch 9: 393778.5\n",
            "Loss after epoch 10: 392093.5\n",
            "Loss after epoch 11: 383913.0\n",
            "Loss after epoch 12: 379567.0\n",
            "Loss after epoch 13: 373229.5\n",
            "Loss after epoch 14: 364925.5\n",
            "Loss after epoch 15: 358811.0\n",
            "Loss after epoch 16: 352606.0\n",
            "Loss after epoch 17: 340987.5\n",
            "Loss after epoch 18: 323450.0\n",
            "Loss after epoch 19: 317781.0\n",
            "Loss after epoch 20: 311589.0\n",
            "Loss after epoch 21: 303876.0\n",
            "Loss after epoch 22: 298406.0\n",
            "Loss after epoch 23: 291872.0\n",
            "Loss after epoch 24: 286732.0\n",
            "Loss after epoch 25: 280743.0\n",
            "Loss after epoch 26: 274962.0\n",
            "Loss after epoch 27: 271280.0\n",
            "Loss after epoch 28: 266013.0\n",
            "Loss after epoch 29: 262725.0\n",
            "Loss after epoch 30: 258685.0\n",
            "Loss after epoch 31: 254612.0\n",
            "Loss after epoch 32: 249746.0\n",
            "Loss after epoch 33: 247071.0\n",
            "Loss after epoch 34: 242721.0\n",
            "Loss after epoch 35: 240506.0\n",
            "Loss after epoch 36: 238753.0\n",
            "Loss after epoch 37: 235335.0\n",
            "Loss after epoch 38: 231356.0\n",
            "Loss after epoch 39: 230840.0\n",
            "Loss after epoch 40: 228144.0\n",
            "Loss after epoch 41: 225195.0\n",
            "Loss after epoch 42: 223693.0\n",
            "Loss after epoch 43: 221128.0\n",
            "Loss after epoch 44: 219085.0\n",
            "Loss after epoch 45: 217426.0\n",
            "Loss after epoch 46: 216142.0\n",
            "Loss after epoch 47: 213800.0\n",
            "Loss after epoch 48: 211626.0\n",
            "Loss after epoch 49: 211139.0\n",
            "Loss after epoch 50: 201464.0\n",
            "Loss after epoch 51: 172180.0\n",
            "Loss after epoch 52: 172686.0\n",
            "Loss after epoch 53: 171070.0\n",
            "Loss after epoch 54: 169306.0\n",
            "Loss after epoch 55: 166818.0\n",
            "Loss after epoch 56: 166718.0\n",
            "Loss after epoch 57: 165014.0\n",
            "Loss after epoch 58: 163680.0\n",
            "Loss after epoch 59: 163730.0\n",
            "Loss after epoch 60: 162934.0\n",
            "Loss after epoch 61: 161278.0\n",
            "Loss after epoch 62: 160970.0\n",
            "Loss after epoch 63: 159836.0\n",
            "Loss after epoch 64: 158878.0\n",
            "Loss after epoch 65: 158530.0\n",
            "Loss after epoch 66: 158808.0\n",
            "Loss after epoch 67: 156094.0\n",
            "Loss after epoch 68: 156464.0\n",
            "Loss after epoch 69: 155172.0\n",
            "Loss after epoch 70: 155160.0\n",
            "Loss after epoch 71: 154194.0\n",
            "Loss after epoch 72: 153484.0\n",
            "Loss after epoch 73: 153152.0\n",
            "Loss after epoch 74: 152526.0\n",
            "Loss after epoch 75: 151838.0\n",
            "Loss after epoch 76: 151930.0\n",
            "Loss after epoch 77: 151588.0\n",
            "Loss after epoch 78: 150644.0\n",
            "Loss after epoch 79: 151396.0\n",
            "Loss after epoch 80: 149404.0\n",
            "Loss after epoch 81: 149258.0\n",
            "Loss after epoch 82: 150078.0\n",
            "Loss after epoch 83: 149244.0\n",
            "Loss after epoch 84: 149094.0\n",
            "Loss after epoch 85: 147424.0\n",
            "Loss after epoch 86: 147816.0\n",
            "Loss after epoch 87: 147748.0\n",
            "Loss after epoch 88: 147078.0\n",
            "Loss after epoch 89: 146236.0\n",
            "Loss after epoch 90: 145984.0\n",
            "Loss after epoch 91: 145164.0\n",
            "Loss after epoch 92: 146516.0\n",
            "Loss after epoch 93: 146228.0\n",
            "Loss after epoch 94: 145622.0\n",
            "Loss after epoch 95: 144952.0\n",
            "Loss after epoch 96: 145444.0\n",
            "Loss after epoch 97: 144602.0\n",
            "Loss after epoch 98: 145808.0\n",
            "Loss after epoch 99: 144386.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5214705, 5551500)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Entrenamos el modelo generador de vectores\n",
        "w2v_model.train(sentence_tokens,\n",
        "                 total_examples=w2v_model.corpus_count,\n",
        "                 epochs=100,\n",
        "                 compute_loss = True,\n",
        "                 callbacks=[callback()]\n",
        "                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43b83fc",
      "metadata": {
        "id": "e43b83fc"
      },
      "source": [
        "### Términos de interés\n",
        "Se buscará los términos más similares a algunas de las palabras más frecuentes y relevantes de la obra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "03130322",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03130322",
        "outputId": "6be44457-0ba3-4584-b299-4c1b9dfb2ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palabras similares a 'señora':\n",
            "  bennet: 0.885\n",
            "  embargó: 0.742\n",
            "  chispas: 0.720\n",
            "  dependes: 0.715\n",
            "  insisto: 0.697\n",
            "\n",
            "Palabras similares a 'matrimonio':\n",
            "  perfidia: 0.679\n",
            "  notificase: 0.650\n",
            "  contraiga: 0.638\n",
            "  rigida: 0.612\n",
            "  ansía: 0.611\n",
            "\n",
            "Palabras similares a 'elizabeth':\n",
            "  ruborizada: 0.773\n",
            "  afirmarle: 0.768\n",
            "  empaque: 0.764\n",
            "  elegantemente: 0.762\n",
            "  darcy: 0.761\n",
            "\n",
            "Palabras similares a 'familia':\n",
            "  supiesen: 0.613\n",
            "  emparentado: 0.612\n",
            "  esperándolos: 0.597\n",
            "  deseaba: 0.596\n",
            "  verían: 0.577\n",
            "\n",
            "Palabras similares a 'hermana':\n",
            "  decidimos: 0.653\n",
            "  implicaban: 0.626\n",
            "  entristecían: 0.596\n",
            "  inclinado: 0.588\n",
            "  reanimase: 0.574\n"
          ]
        }
      ],
      "source": [
        "key_terms = [\"señora\", \"matrimonio\", \"elizabeth\", \"familia\", \"hermana\"]\n",
        "\n",
        "for term in key_terms:\n",
        "    if term in w2v_model.wv:\n",
        "        similar = w2v_model.wv.most_similar(term, topn=5)\n",
        "        print(f\"\\nPalabras similares a '{term}':\")\n",
        "        for word, sim in similar:\n",
        "            print(f\"  {word}: {sim:.3f}\")\n",
        "    else:\n",
        "        print(f\"\\n'{term}' no encontrada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Palabras similares a \"señora\":**  \n",
        "   Las palabras más similares son *bennet* (apellido clave de la familia principal) y verbos en formas conjugadas o participios (*embargó*, *chispas*, *dependes*, *insisto*). Esto sugiere que la palabra \"señora\" está muy asociada a personajes y acciones en el contexto narrativo, reflejando que el modelo capta relaciones de personajes y acciones frecuentes junto con términos formales de tratamiento.\n",
        "\n",
        "- **Palabras similares a \"matrimonio\":**  \n",
        "   Los términos asociados son más abstractos y formales: *perfidia* (traición), *notificase* (verbo en subjuntivo), *contraiga* (verbo legal o formal de \"contraer matrimonio\"), *rígida*, *ansía*. Esto indica que el modelo asocia \"matrimonio\" con conceptos legales, emocionales y formales presentes en el texto.\n",
        "\n",
        "- **Palabras similares a \"elizabeth\":**  \n",
        "   Se destacan términos descriptivos y emocionales como *ruborizada*, *afirmarle*, *elegantemente*, además de la referencia directa a *darcy*. Esto muestra que el modelo reconoce a Elizabeth como un personaje central y la conecta con acciones y estados emocionales que la rodean.\n",
        "\n",
        "- **Palabras similares a \"familia\":**  \n",
        "   Palabras relacionadas con relaciones familiares y estados, como *supiesen*, *emparentado*, *esperándolos*, *deseaba*, *verían*, sugieren que el modelo entiende \"familia\" en su sentido social y afectivo, enlazándola con vínculos y expectativas.\n",
        "\n",
        "- **Palabras similares a \"hermana\":**  \n",
        "   Aquí aparecen verbos y estados emocionales como *decidimos*, *implicaban*, *entristecían*, *inclinado*, *reanimase*. Esto indica que el modelo relaciona \"hermana\" con decisiones, emociones y dinámicas internas, típicas del contexto familiar y personal en la novela.\n",
        "\n",
        "El modelo Word2Vec entrenado sobre *Orgullo y Prejuicio* refleja las relaciones temáticas y emocionales del texto, capturando asociaciones entre personajes, acciones, estados emocionales y conceptos clave de la obra, aunque algunas palabras similares pueden parecer poco intuitivas, probablemente por el contexto lingüístico o frecuencia de uso en el corpus."
      ],
      "metadata": {
        "id": "sqxTrNquDqwL"
      },
      "id": "sqxTrNquDqwL"
    },
    {
      "cell_type": "markdown",
      "id": "ed0ed02a",
      "metadata": {
        "id": "ed0ed02a"
      },
      "source": [
        "### Tests de analogías"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función auxiliar para probar analogías\n",
        "def test_analogy(w2v_model, positive, negative, description):\n",
        "    try:\n",
        "        # Verificar si las palabras están en el vocabulario\n",
        "        for word in positive + negative:\n",
        "            if word not in w2v_model.wv:\n",
        "                raise KeyError(f\"{word}\")\n",
        "\n",
        "        results = w2v_model.wv.most_similar(positive=positive, negative=negative, topn=3)\n",
        "        print(f\"Analogía: {description} =\")\n",
        "        for word, similarity in results:\n",
        "            print(f\"  {word} (Similitud: {similarity:.4f})\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Palabra no encontrada: {e}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Lista de analogías a probar\n",
        "analogies = [\n",
        "    (['longbourn', 'darcy'], ['elizabeth'], 'longbourn - elizabeth + darcy'),\n",
        "    (['jane', 'darcy'], ['elizabeth'], 'jane - elizabeth + darcy'),\n",
        "    (['señora', 'hombre'], ['mujer'], 'señora - mujer + hombre'),\n",
        "    (['esposo', 'mujer'], ['hombre'], 'esposo - hombre + mujer')\n",
        "]\n",
        "\n",
        "# Ejecutar las analogías\n",
        "for positive, negative, description in analogies:\n",
        "    test_analogy(w2v_model, positive, negative, description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO0AUFN7BfAE",
        "outputId": "a0f775be-d1cf-43fa-da34-4fbd0ec7df1b"
      },
      "id": "PO0AUFN7BfAE",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogía: longbourn - elizabeth + darcy =\n",
            "  numeroso (Similitud: 0.6008)\n",
            "  ejerciese (Similitud: 0.5695)\n",
            "  newcastle (Similitud: 0.5604)\n",
            "--------------------------------------------------\n",
            "Analogía: jane - elizabeth + darcy =\n",
            "  bingley (Similitud: 0.8185)\n",
            "  señor (Similitud: 0.6735)\n",
            "  influyen (Similitud: 0.6471)\n",
            "--------------------------------------------------\n",
            "Analogía: señora - mujer + hombre =\n",
            "  angustia (Similitud: 0.5947)\n",
            "  m (Similitud: 0.5328)\n",
            "  bennet (Similitud: 0.5281)\n",
            "--------------------------------------------------\n",
            "Analogía: esposo - hombre + mujer =\n",
            "  indirecta (Similitud: 0.6026)\n",
            "  molestara (Similitud: 0.5587)\n",
            "  revela (Similitud: 0.5509)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **longbourn - elizabeth + darcy**\n",
        "  - Palabras resultantes: *numeroso*, *ejerciese*, *newcastle*\n",
        "  - Interpretación: La analogía no produce términos directamente relacionados con los personajes ni con ubicaciones clave, salvo posiblemente *newcastle*. El modelo puede estar confundido por la baja frecuencia de ciertas combinaciones, reflejando que el contexto semántico entre estas tres palabras es débil en el corpus reducido.\n",
        "\n",
        "- **jane - elizabeth + darcy**\n",
        "  - Palabras resultantes: *bingley*, *señor*, *influyen*\n",
        "  - Interpretación: Esta analogía es bastante coherente. *Bingley* es la pareja de Jane, así como *Darcy* lo es de Elizabeth, lo que sugiere que el modelo ha captado correctamente esta relación paralela. El resto de palabras reflejan interacciones o dinámicas típicas en el entorno social de la novela.\n",
        "\n",
        "- **señora - mujer + hombre**\n",
        "  - Palabras resultantes: *angustia*, *m*, *bennet*\n",
        "  - Interpretación: Aunque el resultado incluye *bennet*, que podría ser coherente (por *señora Bennet*), las demás palabras no representan una conversión clara de género. El modelo parece captar el entorno emocional o social de la palabra “señora”, pero no logra ajustar bien la dimensión de género con la operación semántica esperada.\n",
        "\n",
        "- **esposo - hombre + mujer**\n",
        "  - Palabras resultantes: *indirecta*, *molestara*, *revela*\n",
        "  - Interpretación: La analogía no arroja palabras femeninas ni relacionadas claramente con el rol de “esposa”. Esto sugiere que el modelo no ha captado con fuerza la estructura de género entre “esposo”, “hombre” y “mujer”, posiblemente por falta de ejemplos suficientes o balanceados en el corpus.\n",
        "\n",
        "Las analogías que involucran relaciones entre personajes (*Jane*, *Elizabeth*, *Darcy*) son las que arrojan resultados más coherentes, mientras que aquellas que intentan capturar relaciones de género o roles sociales (*señora*, *esposo*) muestran más ruido semántico.\n"
      ],
      "metadata": {
        "id": "UM6IQWofEqRl"
      },
      "id": "UM6IQWofEqRl"
    },
    {
      "cell_type": "markdown",
      "id": "c2afffcf",
      "metadata": {
        "id": "c2afffcf"
      },
      "source": [
        "### Gráficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2c750791",
      "metadata": {
        "id": "2c750791"
      },
      "outputs": [],
      "source": [
        "def reduce_dimensions(model, num_dimensions = 2 ):\n",
        "\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)\n",
        "\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    return vectors, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6d149b09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6d149b09",
        "outputId": "c5808cc3-c147-4ec9-aced-5b3397fa7876"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"717b9bea-5d14-42e9-a062-87334a95b8bd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"717b9bea-5d14-42e9-a062-87334a95b8bd\")) {                    Plotly.newPlot(                        \"717b9bea-5d14-42e9-a062-87334a95b8bd\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"elizabeth\",\"darcy\",\"señor\",\"si\",\"tan\",\"bingley\",\"señora\",\"jane\",\"bennet\",\"dijo\",\"usted\",\"wickham\",\"ser\",\"señorita\",\"collins\",\"casa\",\"aunque\",\"dos\",\"hermana\",\"bien\",\"lydia\",\"catherine\",\"después\",\"nunca\",\"podía\",\"sólo\",\"vez\",\"pues\",\"tiempo\",\"familia\",\"todas\",\"siempre\",\"menos\",\"sido\",\"puede\",\"así\",\"ver\",\"lady\",\"mejor\",\"ahora\",\"hacer\",\"padre\",\"decir\",\"modo\",\"día\",\"madre\",\"hecho\",\"carta\",\"haber\",\"hablar\",\"mismo\",\"cómo\",\"toda\",\"gardiner\",\"cosa\",\"tener\",\"puedo\",\"cuanto\",\"mal\",\"hombre\",\"misma\",\"parte\",\"longbourn\",\"iba\",\"charlotte\",\"entonces\",\"querida\",\"creo\",\"tal\",\"ninguna\",\"cosas\",\"pensar\",\"hermanas\",\"veces\",\"hijas\",\"saber\",\"hizo\",\"mientras\",\"hija\",\"demasiado\",\"mañana\",\"mayor\",\"pronto\",\"nadie\",\"gran\",\"lucas\",\"mundo\",\"mujer\",\"pesar\",\"hacia\",\"fin\",\"visto\",\"londres\",\"hacía\",\"pudo\",\"casi\",\"aquella\",\"parecía\",\"ningún\",\"coronel\"],\"x\":[-5.6449966,10.129645,11.608062,15.102638,-22.251236,9.756084,21.785275,-5.859291,21.782495,-12.140203,21.111526,10.373141,20.970846,21.578516,27.07784,44.550457,8.020883,51.13786,-9.135201,33.088078,29.925137,35.6553,56.130085,-10.326332,11.242617,3.3284638,56.787746,29.700546,11.209645,-25.691708,-35.019558,-1.6991566,2.6352675,16.969368,9.022956,20.581734,-1.5541246,35.65267,10.095154,2.0791895,41.80784,4.6406293,8.640458,15.286613,65.501274,9.643505,8.167984,6.643021,10.085596,-27.266958,20.705963,24.887941,-36.876156,22.064766,37.298565,-17.840548,11.2513075,31.836174,30.206192,26.935268,-29.129578,-33.017624,44.574078,43.768864,12.639352,20.246773,13.273476,-15.067301,-57.309387,-0.8778784,-13.371923,24.159933,52.81461,-22.783945,48.221436,36.90174,4.5294495,45.386124,34.59502,-18.455917,64.50952,-45.146603,46.444355,8.2819395,-41.642673,25.187458,-0.78120536,21.483227,-14.264913,-54.476326,6.3542976,-3.2628298,43.734295,-12.505308,-1.7399206,-52.883892,-31.116426,-9.886134,16.273714,66.82536],\"xaxis\":\"x\",\"y\":[35.228012,24.337418,23.280136,-9.529346,-45.04509,24.436197,43.564327,34.69779,43.585087,37.068333,9.793095,24.994637,-33.78318,48.954483,27.681705,19.601448,-46.17953,-21.733435,30.588865,-8.350193,0.24053383,61.71188,31.76067,-43.94223,-64.62461,-47.438236,49.424488,-18.188833,-14.871106,-3.6069376,-55.337322,-31.482735,-4.208504,-51.33862,-23.032005,1.0993437,14.395669,61.712254,-41.063934,-14.775113,-22.393246,16.54973,5.595404,-39.047157,4.5865655,77.77242,-55.68033,42.410263,-58.817383,41.475464,62.64716,1.1131256,-5.483689,34.782547,-30.601067,-73.64226,1.3180507,25.748388,-34.58089,-45.570614,23.849485,-14.597204,23.05535,-8.8835,39.628986,12.5679655,36.814518,69.891655,-7.952857,-83.63006,-35.973827,-50.359047,-27.814074,4.140809,-32.722862,6.7788143,63.208015,39.502113,-10.603776,47.81524,-1.1749736,-40.676304,31.797785,-8.066347,-8.95473,57.496723,-37.37311,13.271202,-39.744934,22.40873,7.5124345,-44.757343,-6.3133097,-2.3076475,19.047003,21.841925,-3.2880812,-3.407255,-24.97578,34.218452],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('717b9bea-5d14-42e9-a062-87334a95b8bd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Graficar los embedddings en 2D\n",
        "\n",
        "vecs, labels = reduce_dimensions(w2v_model)\n",
        "\n",
        "MAX_WORDS=100\n",
        "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
        "fig.show(renderer=\"colab\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# También se pueden guardar los vectores y labels como tsv para graficar en\n",
        "# http://projector.tensorflow.org/\n",
        "\n",
        "\n",
        "vectors = np.asarray(w2v_model.wv.vectors)\n",
        "labels = list(w2v_model.wv.index_to_key)\n",
        "\n",
        "np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
        "\n",
        "with open(\"labels.tsv\", \"w\") as fp:\n",
        "    for item in labels:\n",
        "        fp.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "RKIpPiLsJ0Jc"
      },
      "id": "RKIpPiLsJ0Jc",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb5e634e",
      "metadata": {
        "id": "eb5e634e"
      },
      "source": [
        "### Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo de word embeddings entrenado con Word2Vec sobre el texto de Orgullo y Prejuicio en español capturó adecuadamente las relaciones contextuales y temáticas propias de la obra. Las palabras similares reflejan asociaciones relevantes con personajes, acciones y estados emocionales presentes en la narrativa.\n",
        "\n",
        "Sin embargo, los tests de analogías mostraron resultados mixtos. Las analogías relacionadas con relaciones entre personajes principales obtuvieron respuestas más coherentes, mientras que aquellas que intentaban representar relaciones de género o roles sociales no lograron resultados claros ni consistentes. Esto indica que, aunque el modelo aprende asociaciones locales y específicas del corpus, tiene dificultades para generalizar relaciones semánticas más abstractas o universales.\n",
        "\n",
        "En conclusión, entrenar embeddings en un corpus literario pequeño y especializado es útil para captar el contexto específico, pero limita la capacidad del modelo para tareas que requieren generalización semántica amplia. Para superar esta limitación, sería recomendable utilizar modelos preentrenados en grandes corpus o combinar embeddings específicos con representaciones más generales.\n"
      ],
      "metadata": {
        "id": "TwwtR-0fO2Ic"
      },
      "id": "TwwtR-0fO2Ic"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CEIA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}